{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M4dVuL_UMWff",
    "outputId": "9a98a8d8-4b3c-4624-f124-7d462ab34638"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'gen-vis-assignment-4'...\n",
      "remote: Enumerating objects: 18, done.\u001b[K\n",
      "remote: Counting objects: 100% (18/18), done.\u001b[K\n",
      "remote: Compressing objects: 100% (14/14), done.\u001b[K\n",
      "remote: Total 18 (delta 9), reused 13 (delta 4), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (18/18), 3.43 MiB | 20.53 MiB/s, done.\n",
      "Resolving deltas: 100% (9/9), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/RahulV2478/gen-vis-assignment-4.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jmtIGnBSPrcI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ghBxrOPbM44-",
    "outputId": "da6f3a83-207a-4ab8-a86c-a1d686354a5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/gen-vis-assignment-4\n",
      "Assignment4.ipynb  Assignment4.pdf  DDPM.png  diffusion.py\n"
     ]
    }
   ],
   "source": [
    "%cd gen-vis-assignment-4\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zsgaTzLWPtkH",
    "outputId": "0e72e577-c741-470c-819a-6bd0bc9b7a3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remote: Enumerating objects: 4, done.\u001b[K\n",
      "remote: Counting objects:  25% (1/4)\u001b[K\r",
      "remote: Counting objects:  50% (2/4)\u001b[K\r",
      "remote: Counting objects:  75% (3/4)\u001b[K\r",
      "remote: Counting objects: 100% (4/4)\u001b[K\r",
      "remote: Counting objects: 100% (4/4), done.\u001b[K\n",
      "remote: Compressing objects:  50% (1/2)\u001b[K\r",
      "remote: Compressing objects: 100% (2/2)\u001b[K\r",
      "remote: Compressing objects: 100% (2/2), done.\u001b[K\n",
      "remote: Total 3 (delta 1), reused 3 (delta 1), pack-reused 0 (from 0)\u001b[K\n",
      "Unpacking objects:  33% (1/3)\r",
      "Unpacking objects:  66% (2/3)\r",
      "Unpacking objects: 100% (3/3)\r",
      "Unpacking objects: 100% (3/3), 1.32 KiB | 1.32 MiB/s, done.\n",
      "From https://github.com/RahulV2478/gen-vis-assignment-4\n",
      "   ceb8016..8aa1854  main       -> origin/main\n",
      "Updating ceb8016..8aa1854\n",
      "Fast-forward\n",
      " train_diffusion.py | 96 \u001b[32m++++++++++++++++++++++++++++++++++++++++++++++++++++++\u001b[m\n",
      " 1 file changed, 96 insertions(+)\n",
      " create mode 100644 train_diffusion.py\n"
     ]
    }
   ],
   "source": [
    "!git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8c9a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile diffusion.py\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.SiLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "class DiffusionModel(nn.Module):\n",
    "    def __init__(self, image_size, channels=None, hidden_dims=None, channles=None):\n",
    "        super().__init__()\n",
    "\n",
    "        if channels is None:\n",
    "            channels = channles\n",
    "        if channels is None:\n",
    "            raise ValueError(\"You must provide `channels` or `channles`.\")\n",
    "\n",
    "        if hidden_dims is None:\n",
    "            hidden_dims = [32, 64]\n",
    "\n",
    "        self.image_size = image_size\n",
    "        self.in_channels = channels\n",
    "        self.hidden_dims = hidden_dims\n",
    "\n",
    "        self.max_time = 1000\n",
    "        time_dim = hidden_dims[0]\n",
    "\n",
    "        self.time_embed = nn.Embedding(self.max_time, time_dim)\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            nn.Linear(time_dim, time_dim * 4),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_dim * 4, time_dim),\n",
    "        )\n",
    "\n",
    "        self.init_conv = nn.Conv2d(self.in_channels, hidden_dims[0], kernel_size=3, padding=1)\n",
    "\n",
    "        self.down_blocks = nn.ModuleList()\n",
    "        self.downsamples = nn.ModuleList()\n",
    "\n",
    "        in_ch = hidden_dims[0]\n",
    "        for i, out_ch in enumerate(hidden_dims):\n",
    "            self.down_blocks.append(DoubleConv(in_ch, out_ch))\n",
    "            if i != len(hidden_dims) - 1:\n",
    "                self.downsamples.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "            in_ch = out_ch\n",
    "\n",
    "        bottleneck_channels = hidden_dims[-1] * 2\n",
    "        self.bottleneck = DoubleConv(hidden_dims[-1], bottleneck_channels)\n",
    "\n",
    "        decoder_dims = list(reversed(hidden_dims[:-1]))\n",
    "\n",
    "        self.up_trans = nn.ModuleList()\n",
    "        self.up_blocks = nn.ModuleList()\n",
    "        self.time_projs = nn.ModuleList()\n",
    "\n",
    "        in_ch = bottleneck_channels\n",
    "        for out_ch in decoder_dims:\n",
    "            self.up_trans.append(\n",
    "                nn.ConvTranspose2d(in_ch, out_ch, kernel_size=2, stride=2)\n",
    "            )\n",
    "            self.up_blocks.append(DoubleConv(out_ch * 2, out_ch))\n",
    "            self.time_projs.append(nn.Linear(time_dim, out_ch))\n",
    "            in_ch = out_ch\n",
    "\n",
    "        self.final_conv = nn.Conv2d(hidden_dims[0], self.in_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        if t.dim() == 0:\n",
    "            t = t.unsqueeze(0)\n",
    "        t = t.long().clamp(max=self.max_time - 1)\n",
    "        t_embed = self.time_embed(t)\n",
    "        t_embed = self.time_mlp(t_embed)\n",
    "\n",
    "        out = self.init_conv(x)\n",
    "\n",
    "        skips = []\n",
    "        for i, down_block in enumerate(self.down_blocks):\n",
    "            out = down_block(out)\n",
    "            if i != len(self.down_blocks) - 1:\n",
    "                skips.append(out)\n",
    "                out = self.downsamples[i](out)\n",
    "\n",
    "        out = self.bottleneck(out)\n",
    "\n",
    "        for i, up in enumerate(self.up_trans):\n",
    "            out = up(out)\n",
    "            skip = skips[-(i + 1)]\n",
    "\n",
    "            time_feat = self.time_projs[i](t_embed)\n",
    "            time_feat = time_feat.view(time_feat.size(0), -1, 1, 1)\n",
    "            out = out + time_feat\n",
    "\n",
    "            out = torch.cat([out, skip], dim=1)\n",
    "            out = self.up_blocks[i](out)\n",
    "\n",
    "        out = self.final_conv(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class DiffusionProcess:\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_size,\n",
    "        channels,\n",
    "        hidden_dims=None,\n",
    "        beta_start=1e-4,\n",
    "        beta_end=0.02,\n",
    "        noise_steps=1000,\n",
    "        beta_schedule=\"linear\",\n",
    "        device=torch.device(\"cpu\"),\n",
    "    ):\n",
    "        if hidden_dims is None:\n",
    "            hidden_dims = [32, 64]\n",
    "\n",
    "        self.image_size = image_size\n",
    "        self.channels = channels\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.beta_start = beta_start\n",
    "        self.beta_end = beta_end\n",
    "        self.noise_steps = noise_steps\n",
    "\n",
    "        if device is None:\n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.device = device\n",
    "\n",
    "        steps = torch.linspace(0.0, 1.0, noise_steps, device=self.device)\n",
    "\n",
    "        if beta_schedule == \"linear\":\n",
    "            betas = torch.linspace(beta_start, beta_end, noise_steps, device=self.device)\n",
    "        elif beta_schedule == \"cosine\":\n",
    "            cosine = 0.5 * (1.0 - torch.cos(torch.pi * steps))\n",
    "            betas = beta_start + (beta_end - beta_start) * cosine\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown beta_schedule: {beta_schedule}\")\n",
    "\n",
    "        self.betas = betas\n",
    "        self.alphas = 1.0 - self.betas\n",
    "        self.alpha_cumprod = torch.cumprod(self.alphas, dim=0)\n",
    "\n",
    "        self.sqrt_alpha_cumprod = torch.sqrt(self.alpha_cumprod)\n",
    "        self.sqrt_one_minus_alpha_cumprod = torch.sqrt(1.0 - self.alpha_cumprod)\n",
    "\n",
    "        self.model = DiffusionModel(\n",
    "            image_size=image_size,\n",
    "            channels=channels,\n",
    "            hidden_dims=hidden_dims,\n",
    "        ).to(self.device)\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=2e-4)\n",
    "\n",
    "    def add_noise(self, x, t):\n",
    "        x = x.to(self.device)\n",
    "        t = t.to(self.device).long()\n",
    "\n",
    "        sqrt_alpha_hat = self.sqrt_alpha_cumprod[t].view(-1, 1, 1, 1)\n",
    "        sqrt_one_minus_alpha_hat = self.sqrt_one_minus_alpha_cumprod[t].view(-1, 1, 1, 1)\n",
    "\n",
    "        noise = torch.randn_like(x)\n",
    "        noisy_x = sqrt_alpha_hat * x + sqrt_one_minus_alpha_hat * noise\n",
    "        return noisy_x, noise\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample_from_noise(self, x_T):\n",
    "        self.model.eval()\n",
    "        x = x_T.to(self.device)\n",
    "\n",
    "        num_samples = x.size(0)\n",
    "        for t in reversed(range(self.noise_steps)):\n",
    "            t_batch = torch.full((num_samples,), t, device=self.device, dtype=torch.long)\n",
    "            eps_theta = self.model(x, t_batch)\n",
    "\n",
    "            beta_t = self.betas[t]\n",
    "            alpha_t = self.alphas[t]\n",
    "            alpha_hat_t = self.alpha_cumprod[t]\n",
    "\n",
    "            coef1 = 1.0 / torch.sqrt(alpha_t)\n",
    "            coef2 = (1.0 - alpha_t) / torch.sqrt(1.0 - alpha_hat_t)\n",
    "\n",
    "            x = coef1 * (x - coef2 * eps_theta)\n",
    "\n",
    "            if t > 0:\n",
    "                sigma_t = torch.sqrt(beta_t)\n",
    "                x = x + sigma_t * torch.randn_like(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, num_samples=16):\n",
    "        x_T = torch.randn(\n",
    "            num_samples,\n",
    "            self.channels,\n",
    "            self.image_size,\n",
    "            self.image_size,\n",
    "            device=self.device,\n",
    "        )\n",
    "        return self.sample_from_noise(x_T)\n",
    "\n",
    "    def train_step(self, x):\n",
    "        self.model.train()\n",
    "        x = x.to(self.device)\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        t = torch.randint(\n",
    "            low=0,\n",
    "            high=self.noise_steps,\n",
    "            size=(batch_size,),\n",
    "            device=self.device,\n",
    "        )\n",
    "\n",
    "        noisy_x, noise = self.add_noise(x, t)\n",
    "        pred_noise = self.model(noisy_x, t)\n",
    "\n",
    "        loss = F.mse_loss(pred_noise, noise)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return float(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6f6f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile train_diffusion.py\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from diffusion import DiffusionProcess\n",
    "\n",
    "\n",
    "def get_mnist_dataloader(batch_size=128, image_size=28):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(image_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "\n",
    "    train_dataset = datasets.MNIST(\n",
    "        root=\"./data\",\n",
    "        train=True,\n",
    "        transform=transform,\n",
    "        download=True,\n",
    "    )\n",
    "\n",
    "    dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=2,\n",
    "        drop_last=True,\n",
    "    )\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "def train_diffusion(\n",
    "    num_epochs=5,\n",
    "    batch_size=128,\n",
    "    noise_steps=1000,\n",
    "    device=None,\n",
    "    sample_every=1,\n",
    "    num_sample_images=16,\n",
    "    out_dir=\"outputs\",\n",
    "    beta_schedule=\"linear\",\n",
    "):\n",
    "    if device is None:\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    device = torch.device(device)\n",
    "\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    dataloader = get_mnist_dataloader(batch_size=batch_size, image_size=28)\n",
    "\n",
    "    diffusion = DiffusionProcess(\n",
    "        image_size=28,\n",
    "        channels=1,\n",
    "        noise_steps=noise_steps,\n",
    "        beta_start=1e-4,\n",
    "        beta_end=0.02,\n",
    "        beta_schedule=beta_schedule,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    loss_history = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for x, _ in dataloader:\n",
    "            loss = diffusion.train_step(x)\n",
    "            running_loss += loss\n",
    "\n",
    "        avg_loss = running_loss / len(dataloader)\n",
    "        loss_history.append(avg_loss)\n",
    "        print(f\"[Epoch {epoch+1}/{num_epochs}] loss={avg_loss:.4f}\")\n",
    "\n",
    "        if (epoch + 1) % sample_every == 0:\n",
    "            diffusion.model.eval()\n",
    "            with torch.no_grad():\n",
    "                samples = diffusion.sample(num_samples=num_sample_images)\n",
    "                samples = (samples.clamp(-1, 1) + 1) / 2.0\n",
    "                save_path = os.path.join(out_dir, f\"samples_epoch_{epoch+1}.png\")\n",
    "                save_image(samples, save_path, nrow=int(num_sample_images ** 0.5))\n",
    "                print(f\"Saved samples to {save_path}\")\n",
    "\n",
    "    model_path = os.path.join(out_dir, \"diffusion_mnist.pth\")\n",
    "    torch.save(diffusion.model.state_dict(), model_path)\n",
    "    print(f\"Saved model to {model_path}\")\n",
    "\n",
    "    return diffusion, loss_history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KDcJwLd5O1NT"
   },
   "source": [
    "# Assignment 4: Diffusion Model\n",
    "\n",
    "In this assignment, you will implement a diffusion model from scratch and train it on the MNIST dataset. Diffusion models are a class of generative models that learn to gradually denoise random noise to generate realistic images. This assignment will guide you through the core components and training process of diffusion models.\n",
    "\n",
    "Useful links:\n",
    "1. [What are Diffusion Models?](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/)\n",
    "2. [Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239)\n",
    "\n",
    "Please:\n",
    "* Fill out the code marked with `TODO` or `Your code here`. You are allowed to split functions or visualizations to different files for more flexibility as long as your output includes what we asked.\n",
    "* Reuse or modify visualization code from Assignment 2 for creating necessary visualizations.\n",
    "* Submit the notebook with all original outputs. If the output is included from another file, please include them into your folder.\n",
    "* Answer questions at the end of the notebook. Write your answere in the notebook.\n",
    "\n",
    "**Please reserve enough time for this assignment given the potential amount of time for training.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "BGCXFVtoO6JL"
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sQtbl8IrO89p"
   },
   "source": [
    "## Part 1: Implementing the U-Net (30 pt)\n",
    "\n",
    "In this part, you will implement a U-Net style model that serves as the backbone for the diffusion process. The model takes noisy images and their corresponding timesteps as input and predicts the noise that was added to the original images.\n",
    "\n",
    "Please fill out the code in `diffusion.DiffusionModel` then run the following code for test. For the time embedding, you can only use one embedding layer and concatenate it with the feature. The attention layer is not enforced given the computation resource."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LLEht6ZeO9x3",
    "outputId": "c3e4102d-df87-4520-87e8-88a4491fd87e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DiffusionModel implementation is correct!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from diffusion import DiffusionModel\n",
    "\n",
    "def check_diffusion_model(model_class):\n",
    "    \"\"\"Verify that the DiffusionModel class is correctly implemented.\"\"\"\n",
    "    try:\n",
    "        channels = 1\n",
    "        image_size = 28\n",
    "        noise_steps = 1000\n",
    "        model = model_class(image_size=image_size, channles=channels)\n",
    "\n",
    "        # Test forward pass with random inputs\n",
    "        batch_size = 4\n",
    "        x = torch.randn(batch_size, channels, image_size, image_size)\n",
    "        t = torch.randint(0, noise_steps, (batch_size,))\n",
    "\n",
    "        output = model(x, t)\n",
    "\n",
    "        # Check output shape\n",
    "        expected_shape = (batch_size, channels, image_size, image_size)\n",
    "        assert output.shape == expected_shape, f\"Expected output shape {expected_shape}, got {output.shape}\"\n",
    "\n",
    "        print(\"DiffusionModel implementation is correct!\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"DiffusionModel check failed: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "check_diffusion_model(DiffusionModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6kF6ANcpSROT"
   },
   "source": [
    "## Part 2: Implementing the Diffusion Process (30 pt)\n",
    "\n",
    "In this part, you will implement the core diffusion process, including the forward diffusion (adding noise) and the denoising process. This includes setting up the noise schedule and implementing functions for noise addition and sampling.\n",
    "\n",
    "Please fill out the code in `diffusion.DiffusionProcess` then run the following code for test. Note that this test only tests the correctness of the output format. You need to be careful about the actual math."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L-My_sNaSTUV",
    "outputId": "a68b9d5f-6e07-4c48-b0d3-5e84a2549094"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DiffusionProcess implementation is correct!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from diffusion import DiffusionProcess\n",
    "\n",
    "def check_diffusion_process(diffusion_class):\n",
    "    \"\"\"Verify that the DiffusionProcess class is correctly implemented.\"\"\"\n",
    "    try:\n",
    "        channels = 1\n",
    "        image_size = 28\n",
    "        noise_steps = 1000\n",
    "        diffusion = diffusion_class(image_size=image_size, channels=channels, noise_steps=noise_steps)\n",
    "\n",
    "        # Test add_noise function\n",
    "        batch_size = 4\n",
    "        x = torch.randn(batch_size, channels, image_size, image_size)\n",
    "        t = torch.randint(0, noise_steps, (batch_size,))\n",
    "\n",
    "        noisy_x, noise = diffusion.add_noise(x, t)\n",
    "        assert noisy_x.shape == x.shape, f\"Expected noisy_x shape {x.shape}, got {noisy_x.shape}\"\n",
    "        assert noise.shape == x.shape, f\"Expected noise shape {x.shape}, got {noise.shape}\"\n",
    "\n",
    "        # Test train_step function\n",
    "        loss = diffusion.train_step(x)\n",
    "        assert isinstance(loss, float), f\"Expected loss to be a float, got {type(loss)}\"\n",
    "\n",
    "        print(\"DiffusionProcess implementation is correct!\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"DiffusionProcess check failed: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "check_diffusion_process(DiffusionProcess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wcODBM5PSLPX"
   },
   "source": [
    "## Part 3: Training and Sampling (20 points)\n",
    "\n",
    "In this part, you will implement the training loop for the diffusion model and the functions for generating and visualizing samples. Please try to follow the assignment you have written and use the `DiffusionModel`  and `DiffusionProcess` above for write your training function. You should write your training code in a standalone python file.\n",
    "\n",
    "Please include the training curves and the sampled results below. You can reuse the visualization code we provided in the GAN assignment.\n",
    "\n",
    "You can include an image like:\n",
    "\n",
    "![image](./DDPM.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OeKXl2heSt7r",
    "outputId": "897f27be-08ed-450a-ff04-3da3d4009481"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/20] loss=0.1005\n",
      "Saved samples to outputs/samples_epoch_1.png\n",
      "[Epoch 2/20] loss=0.0528\n",
      "Saved samples to outputs/samples_epoch_2.png\n",
      "[Epoch 3/20] loss=0.0472\n",
      "Saved samples to outputs/samples_epoch_3.png\n",
      "[Epoch 4/20] loss=0.0442\n",
      "Saved samples to outputs/samples_epoch_4.png\n",
      "[Epoch 5/20] loss=0.0417\n",
      "Saved samples to outputs/samples_epoch_5.png\n",
      "[Epoch 6/20] loss=0.0399\n",
      "Saved samples to outputs/samples_epoch_6.png\n",
      "[Epoch 7/20] loss=0.0388\n",
      "Saved samples to outputs/samples_epoch_7.png\n",
      "[Epoch 8/20] loss=0.0379\n",
      "Saved samples to outputs/samples_epoch_8.png\n",
      "[Epoch 9/20] loss=0.0373\n",
      "Saved samples to outputs/samples_epoch_9.png\n",
      "[Epoch 10/20] loss=0.0363\n",
      "Saved samples to outputs/samples_epoch_10.png\n",
      "[Epoch 11/20] loss=0.0356\n",
      "Saved samples to outputs/samples_epoch_11.png\n",
      "[Epoch 12/20] loss=0.0350\n",
      "Saved samples to outputs/samples_epoch_12.png\n",
      "[Epoch 13/20] loss=0.0339\n",
      "Saved samples to outputs/samples_epoch_13.png\n",
      "[Epoch 14/20] loss=0.0341\n",
      "Saved samples to outputs/samples_epoch_14.png\n",
      "[Epoch 15/20] loss=0.0330\n",
      "Saved samples to outputs/samples_epoch_15.png\n",
      "[Epoch 16/20] loss=0.0335\n",
      "Saved samples to outputs/samples_epoch_16.png\n",
      "[Epoch 17/20] loss=0.0323\n",
      "Saved samples to outputs/samples_epoch_17.png\n",
      "[Epoch 18/20] loss=0.0320\n",
      "Saved samples to outputs/samples_epoch_18.png\n",
      "[Epoch 19/20] loss=0.0316\n",
      "Saved samples to outputs/samples_epoch_19.png\n",
      "[Epoch 20/20] loss=0.0315\n",
      "Saved samples to outputs/samples_epoch_20.png\n",
      "Saved model to outputs/diffusion_mnist.pth\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from train_diffusion import train_diffusion\n",
    "\n",
    "diffusion_linear, loss_linear = train_diffusion(\n",
    "    num_epochs=20,\n",
    "    batch_size=128,\n",
    "    noise_steps=1000,\n",
    "    device=None,\n",
    "    sample_every=1,\n",
    "    num_sample_images=16,\n",
    "    out_dir=\"outputs_linear\",\n",
    "    beta_schedule=\"linear\",\n",
    ")\n",
    "\n",
    "diffusion_cosine, loss_cosine = train_diffusion(\n",
    "    num_epochs=10,\n",
    "    batch_size=128,\n",
    "    noise_steps=1000,\n",
    "    device=None,\n",
    "    sample_every=1,\n",
    "    num_sample_images=16,\n",
    "    out_dir=\"outputs_cosine\",\n",
    "    beta_schedule=\"cosine\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eef343b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.plot(loss_linear, marker=\"o\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Diffusion training loss (linear β)\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6, 3))\n",
    "plt.plot(range(len(loss_linear)), loss_linear, marker=\"o\", label=\"linear β\")\n",
    "plt.plot(range(len(loss_cosine)), loss_cosine, marker=\"o\", label=\"cosine β\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Diffusion training loss: linear vs cosine β\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d22d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "final_samples_path = os.path.join(\"outputs_linear\", \"samples_epoch_20.png\")\n",
    "\n",
    "if os.path.exists(final_samples_path):\n",
    "    img = Image.open(final_samples_path)\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.imshow(img, cmap=\"gray\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Diffusion samples (epoch 20, linear β)\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Could not find:\", final_samples_path)\n",
    "\n",
    "cosine_samples_path = os.path.join(\"outputs_cosine\", \"samples_epoch_10.png\")\n",
    "\n",
    "if os.path.exists(cosine_samples_path):\n",
    "    img = Image.open(cosine_samples_path)\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.imshow(img, cmap=\"gray\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Diffusion samples (epoch 10, cosine β)\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Could not find:\", cosine_samples_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58db2fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = next(diffusion_linear.model.parameters()).device\n",
    "\n",
    "transform = transforms.ToTensor()\n",
    "mnist_train = datasets.MNIST(root=\"./data\", train=True, transform=transform, download=True)\n",
    "loader = DataLoader(mnist_train, batch_size=64, shuffle=True)\n",
    "\n",
    "def compute_loss_by_timestep(diffusion, data_loader, num_batches=2, num_ts=50):\n",
    "    diffusion.model.eval()\n",
    "    noise_steps = diffusion.noise_steps\n",
    "\n",
    "    ts = torch.linspace(0, noise_steps - 1, num_ts).long().to(device)\n",
    "    losses = torch.zeros(num_ts, device=device)\n",
    "    batch_count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (x, _) in enumerate(data_loader):\n",
    "            if batch_idx >= num_batches:\n",
    "                break\n",
    "            x = x.to(device)\n",
    "            b = x.size(0)\n",
    "\n",
    "            for i, t in enumerate(ts):\n",
    "                t_batch = t.repeat(b)\n",
    "                noisy_x, true_noise = diffusion.add_noise(x, t_batch)\n",
    "                pred_noise = diffusion.model(noisy_x, t_batch)\n",
    "                loss = F.mse_loss(pred_noise, true_noise, reduction=\"mean\")\n",
    "                losses[i] += loss\n",
    "\n",
    "            batch_count += 1\n",
    "\n",
    "    losses = (losses / batch_count).cpu()\n",
    "    return ts.cpu(), losses\n",
    "\n",
    "ts, losses = compute_loss_by_timestep(diffusion_linear, loader)\n",
    "\n",
    "plt.figure(figsize=(6, 3))\n",
    "plt.plot(ts, losses, marker=\"o\", linewidth=1)\n",
    "plt.xlabel(\"timestep t\")\n",
    "plt.ylabel(\"avg MSE loss\")\n",
    "plt.title(\"Per-timestep noise-prediction loss (linear β)\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Fqu987XyRIf"
   },
   "source": [
    "## Part 4: Analysis and Visualization (20 points)\n",
    "\n",
    "Answer the question with your analysis. Most of the questions are open-ended. We are looking for yourown observasion from the experiments you did.\n",
    "\n",
    "1. How does the choice of noise schedule (beta values) affect the training stability and sample quality? Try at least one alternative to the linear schedule (e.g., cosine or quadratic) and compare the results.\n",
    "\n",
    "[Answer]:\n",
    "\n",
    "2. Based on your observations, at which timesteps (early, middle, or late in the diffusion process) does the model seem to struggle the most with accurately predicting the noise (looking into loss)? Why do you think this occurs?\n",
    "\n",
    "[Answer]:\n",
    "\n",
    "3. Perform interpolation between two noise vectors and analyze the resulting generated images. Is the transition smooth? What does this tell you about the model's learned latent space?\n",
    "\n",
    "[Answer]:\n",
    "\n",
    "4. Recall Assignment 2, we implemented GAN. compare your diffusion model with GANs in terms of:\n",
    "* Training stability\n",
    "* Sample quality\n",
    "* Diversity of samples\n",
    "* Computational requirements\n",
    "* Anything else you find interesting\n",
    "\n",
    "[Answer]:"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
